# README.md

```markdown
# Classical Chinese BiLSTM - Sentence Segmentation & Punctuation

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.5.1](https://img.shields.io/badge/PyTorch-2.5.1-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

MÃ´ hÃ¬nh BiLSTM+CRF/Linear cho xá»­ lÃ½ vÄƒn báº£n HÃ¡n cá»• (Classical Chinese) vá»›i hai tÃ¡c vá»¥:
- **Sentence Segmentation**: PhÃ¢n Ä‘oáº¡n cÃ¢u theo schema BEMS (Begin, Middle, End, Single)
- **Sentence Punctuation**: Dá»± Ä‘oÃ¡n dáº¥u cÃ¢u (ï¼Œã€ã€‚ï¼šï¼›ï¼Ÿï¼)

---

## ğŸ“‹ Má»¥c lá»¥c

- [Cáº¥u trÃºc Project](#-cáº¥u-trÃºc-project)
- [YÃªu cáº§u Há»‡ thá»‘ng](#-yÃªu-cáº§u-há»‡-thá»‘ng)
- [Setup Environment](#-setup-environment)
- [Cáº¥u hÃ¬nh Training](#ï¸-cáº¥u-hÃ¬nh-training)
- [HÆ°á»›ng dáº«n Sá»­ dá»¥ng](#-hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
- [Quáº£n lÃ½ Jobs](#-quáº£n-lÃ½-jobs)
- [Káº¿t quáº£ Training](#-káº¿t-quáº£-training)
- [Troubleshooting](#-troubleshooting)
- [Examples](#-examples)

---

## ğŸ“ Cáº¥u trÃºc Project

/media02/ddien02/thanhxuan217/main_src/
â”‚
â”œâ”€â”€ README.md                          # ğŸ“– HÆ°á»›ng dáº«n nÃ y
â”œâ”€â”€ requirements.txt                   # ğŸ“¦ Python dependencies
â”‚
â”œâ”€â”€ envs/                              # ğŸ Conda environments (local)
â”‚   â””â”€â”€ classical_chinese/             # Environment cho project nÃ y
â”‚
â”œâ”€â”€ config.slurm                       # âš™ï¸  Cáº¥u hÃ¬nh training
â”œâ”€â”€ train.slurm                        # ğŸš€ SLURM script training
â”œâ”€â”€ evaluate.slurm                     # ğŸ“Š SLURM script evaluation
â”œâ”€â”€ resume.slurm                       # ğŸ”„ SLURM script resume training
â”‚
â”œâ”€â”€ train.py                           # ğŸ“ Training script
â”œâ”€â”€ evaluate.py                        # ğŸ“ˆ Evaluation script
â”‚
â”œâ”€â”€ src/                               # ğŸ’» Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                      # Configuration classes
â”‚   â”œâ”€â”€ dataset.py                     # Dataset & DataLoader
â”‚   â”œâ”€â”€ model.py                       # BiLSTM+CRF/Linear models
â”‚   â”œâ”€â”€ trainer.py                     # Training logic
â”‚   â””â”€â”€ metrics.py                     # Evaluation metrics
â”‚
â”œâ”€â”€ data/                              # ğŸ“š Dá»¯ liá»‡u (JSONL format)
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”œâ”€â”€ val.jsonl
â”‚   â””â”€â”€ test.jsonl
â”‚
â”œâ”€â”€ checkpoints/                       # ğŸ’¾ Model checkpoints
â”‚   â””â”€â”€ {task}/
â”‚       â”œâ”€â”€ best_model.pt
â”‚       â”œâ”€â”€ latest_checkpoint.pt
â”‚       â””â”€â”€ checkpoint_epoch_*.pt
â”‚
â”œâ”€â”€ logs/                              # ğŸ“ Training logs
â”‚   â””â”€â”€ {task}/
â”‚       â”œâ”€â”€ val_metrics_epoch_*.json
â”‚       â””â”€â”€ test_metrics_final.json
â”‚
â”œâ”€â”€ slurm_logs/                        # ğŸ“‹ SLURM output logs
â”‚   â”œâ”€â”€ train_classical_chinese_*.out
â”‚   â””â”€â”€ train_classical_chinese_*.err
â”‚
â””â”€â”€ evaluation_results/                # ğŸ“Š Evaluation results
    â””â”€â”€ {task}/
        â”œâ”€â”€ test_metrics.json
        â””â”€â”€ test_samples.txt
```

---

## ğŸ’» YÃªu cáº§u Há»‡ thá»‘ng

### SLURM Cluster Constraints

| Resource | Limit |
|----------|-------|
| **Maximum jobs/group** | 2 jobs Ä‘á»“ng thá»i |
| **GPU per job** | Maximum 2 GPUs |
| **CPU per job** | Maximum 16 cores |
| **Memory per job** | Maximum 64GB RAM |
| **Time per job** | Maximum 48 hours |
| **Partition** | `gpu` |

### Software Requirements

- **Python**: 3.10+
- **CUDA**: 11.8+
- **Anaconda/Miniconda**: Latest version
- **SLURM**: Workload manager

---

## ğŸš€ Setup Environment

### BÆ°á»›c 1: Khá»Ÿi táº¡o Workspace

```bash
# Di chuyá»ƒn vÃ o workspace
cd /media02/ddien02/thanhxuan217/main_src

# Táº¡o thÆ° má»¥c cho conda environment (local)
mkdir -p envs
```

### BÆ°á»›c 2: Load Anaconda Module

```bash
# TÃ¹y vÃ o há»‡ thá»‘ng, chá»n má»™t trong cÃ¡c cÃ¡ch sau:

# Option 1: System-wide Anaconda
source /opt/anaconda3/etc/profile.d/conda.sh

# Option 2: User Anaconda
source $HOME/anaconda3/etc/profile.d/conda.sh

# Option 3: Miniconda
source $HOME/miniconda3/etc/profile.d/conda.sh

# Verify conda loaded
which conda
# Expected: /path/to/conda
```

### BÆ°á»›c 3: Táº¡o Local Conda Environment

**âš ï¸ QUAN TRá»ŒNG**: Environment pháº£i náº±m trong workspace Ä‘á»ƒ dá»… quáº£n lÃ½.

```bash
# Táº¡o environment trong thÆ° má»¥c envs/
conda create --prefix ./envs/classical_chinese python=3.10 -y

# Activate environment
conda activate ./envs/classical_chinese

# Verify activation
which python
# Expected: /media02/ddien02/thanhxuan217/main_src/envs/classical_chinese/bin/python

python --version
# Expected: Python 3.10.x
```

### BÆ°á»›c 4: CÃ i Ä‘áº·t Dependencies

```bash
# Äáº£m báº£o Ä‘ang á»Ÿ trong workspace vÃ  environment Ä‘Ã£ Ä‘Æ°á»£c activate
cd /media02/ddien02/thanhxuan217/main_src
conda activate ./envs/classical_chinese

# Hoáº·c cÃ i táº¥t cáº£ tá»« requirements.txt
pip install -r requirements.txt
```

### BÆ°á»›c 5: Verify Installation

```bash
# Test PyTorch vÃ  CUDA
python << EOF
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"Number of GPUs: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"GPU 0: {torch.cuda.get_device_name(0)}")
EOF
```

**Expected Output:**
```
PyTorch version: 2.5.1
CUDA available: True
CUDA version: 11.8
Number of GPUs: 2 (or more)
GPU 0: NVIDIA A100-SXM4-40GB (example)
```

```bash
# Test cÃ¡c thÆ° viá»‡n khÃ¡c
python << EOF
import torchcrf
import sklearn
import numpy as np
import pandas as pd
from src.config import LabelConfig
print("âœ“ All dependencies installed successfully!")
print("âœ“ Project modules loaded successfully!")
EOF
```

### BÆ°á»›c 6: Táº¡o Directories

```bash
# Táº¡o cÃ¡c thÆ° má»¥c cáº§n thiáº¿t
mkdir -p data
mkdir -p checkpoints
mkdir -p logs
mkdir -p slurm_logs
mkdir -p evaluation_results

# Verify
ls -la
```

### BÆ°á»›c 7: Chuáº©n bá»‹ Dá»¯ liá»‡u

**Format JSONL** - Má»—i dÃ²ng lÃ  má»™t JSON object:

```jsonl
{"text": "å›ä¸è¦‹å›æœ‰ç–¾è‹¥ä»–æ•…ä¸è¦‹ä½¿è€…", "labels": ["M", "M", "E", "B", "M", "M", "M", "M", "E", "B", "E"]}
{"text": "ä½¿çŠ¬å¤«å—å—è˜äº«ä¹Ÿå¤§å¤«ä¸Šå¿ä¹Ÿ", "labels": ["B", "M", "E", "B", "E", "B", "M", "E", "B", "M", "E"]}
```

**Äáº·t files vÃ o thÆ° má»¥c data:**

```bash
# Copy hoáº·c move data files
cp /path/to/your/train.jsonl data/
cp /path/to/your/val.jsonl data/
cp /path/to/your/test.jsonl data/

# Verify
ls -lh data/
# Expected:
# train.jsonl
# val.jsonl
# test.jsonl
```

**Kiá»ƒm tra format:**

```bash
# Xem 2 dÃ²ng Ä‘áº§u tiÃªn
head -2 data/train.jsonl

# Äáº¿m sá»‘ dÃ²ng
wc -l data/*.jsonl
```

---

## âš™ï¸ Cáº¥u hÃ¬nh Training

### Chá»‰nh sá»­a `config.slurm`

```bash
nano config.slurm
```

### CÃ¡c tham sá»‘ quan trá»ng:

#### 1. Task Configuration

```bash
# Task type: "segmentation" hoáº·c "punctuation"
export TASK="segmentation"

# Model head: true (CRF) hoáº·c false (Linear)
export USE_CRF="true"
```

#### 2. Data Paths

```bash
# ÄÆ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i tá»« WORKSPACE
export TRAIN_DATA="${WORKSPACE}/data/train.jsonl"
export VAL_DATA="${WORKSPACE}/data/val.jsonl"
export TEST_DATA="${WORKSPACE}/data/test.jsonl"
```

#### 3. Model Hyperparameters

```bash
export EMBEDDING_DIM=128      # Character embedding dimension
export HIDDEN_DIM=256         # LSTM hidden dimension
export NUM_LAYERS=2           # Number of LSTM layers
export DROPOUT=0.3            # Dropout rate
```

#### 4. Training Hyperparameters

```bash
export BATCH_SIZE=32          # Batch size per GPU
export NUM_EPOCHS=50          # Total training epochs
export LEARNING_RATE=0.001    # Initial learning rate
export WEIGHT_DECAY=0.00001   # L2 regularization
export GRADIENT_CLIP=5.0      # Gradient clipping threshold
```

#### 5. Resume Training

```bash
# Äá»ƒ trá»‘ng náº¿u train tá»« Ä‘áº§u
export RESUME_CHECKPOINT=""

# Hoáº·c chá»‰ Ä‘á»‹nh checkpoint cá»¥ thá»ƒ Ä‘á»ƒ resume
# export RESUME_CHECKPOINT="${WORKSPACE}/checkpoints/segmentation/latest_checkpoint.pt"
```

#### 6. Other Settings

```bash
export NUM_WORKERS=4          # DataLoader workers
export SEED=42                # Random seed
export NUM_SAMPLES=50         # Sá»‘ samples hiá»ƒn thá»‹ khi evaluate
```

---

## ğŸ“– HÆ°á»›ng dáº«n Sá»­ dá»¥ng

### ğŸ§ª BÆ°á»›c 1: Test vá»›i srun (Báº®T BUá»˜C)

**âš ï¸ QUAN TRá»ŒNG**: 
- LuÃ´n test vá»›i `srun` trÆ°á»›c khi submit batch job
- KhÃ´ng giá»¯ resource quÃ¡ lÃ¢u (< 30 phÃºt)
- Äáº£m báº£o code cháº¡y Ä‘Æ°á»£c trÆ°á»›c khi submit job dÃ i

#### Test Training (1 epoch)

```bash
srun --partition=gpu \
     --gres=gpu:1 \
     --cpus-per-task=8 \
     --mem=32G \
     --time=00:30:00 \
     --pty bash << 'EOF'

# Load conda
source /opt/anaconda3/etc/profile.d/conda.sh
conda activate /media02/ddien02/thanhxuan217/main_src/envs/classical_chinese

# Navigate to workspace
cd /media02/ddien02/thanhxuan217/main_src

# Run test training
python train.py \
    --task segmentation \
    --train_data data/train.jsonl \
    --val_data data/val.jsonl \
    --test_data data/test.jsonl \
    --batch_size 16 \
    --num_epochs 1 \
    --save_dir test_checkpoints \
    --log_dir test_logs

conda deactivate
EOF
```

#### Test Evaluation

```bash
srun --partition=gpu \
     --gres=gpu:1 \
     --cpus-per-task=4 \
     --mem=16G \
     --time=00:15:00 \
     --pty bash << 'EOF'

source /opt/anaconda3/etc/profile.d/conda.sh
conda activate /media02/ddien02/thanhxuan217/main_src/envs/classical_chinese
cd /media02/ddien02/thanhxuan217/main_src

python evaluate.py \
    --checkpoint checkpoints/segmentation/best_model.pt \
    --test_data data/test.jsonl \
    --num_samples 10

conda deactivate
EOF
```

**Náº¿u test thÃ nh cÃ´ng**, tiáº¿n hÃ nh submit batch job.

---

### ğŸš€ BÆ°á»›c 2: Submit Jobs vá»›i sbatch

#### âœ… Kiá»ƒm tra Job Limit

```bash
# Kiá»ƒm tra sá»‘ job Ä‘ang cháº¡y
squeue -u $USER

# Äáº£m báº£o < 2 jobs
# Náº¿u Ä‘Ã£ cÃ³ 2 jobs, Ä‘á»£i má»™t job hoÃ n thÃ nh trÆ°á»›c khi submit job má»›i
```

#### ğŸ“ Training tá»« Ä‘áº§u

```bash
# 1. Kiá»ƒm tra config
cat config.slurm | grep -E "TASK|USE_CRF|BATCH_SIZE|NUM_EPOCHS"

# 2. Submit job
sbatch train.slurm

# 3. Láº¥y Job ID tá»« output
# Submitted batch job 12345

# 4. Monitor job
tail -f slurm_logs/train_classical_chinese_12345.out

# 5. Kiá»ƒm tra job status
squeue -j 12345
```

#### ğŸ”„ Resume Training

```bash
# 1. Kiá»ƒm tra checkpoint tá»“n táº¡i
ls -lh checkpoints/segmentation/latest_checkpoint.pt

# 2. Submit resume job
sbatch resume.slurm

# 3. Monitor
tail -f slurm_logs/resume_train_classical_chinese_*.out
```

#### ğŸ“Š Evaluation Only

```bash
# 1. Kiá»ƒm tra best model
ls -lh checkpoints/segmentation/best_model.pt

# 2. Submit evaluation
sbatch evaluate.slurm

# 3. Monitor
tail -f slurm_logs/eval_classical_chinese_*.out
```

---

## ğŸ® Quáº£n lÃ½ Jobs

### Xem Job Status

```bash
# Xem táº¥t cáº£ jobs cá»§a báº¡n
squeue -u $USER

# Xem chi tiáº¿t hÆ¡n
squeue -u $USER -o "%.18i %.9P %.30j %.8T %.10M %.9l %.6D %R"

# Giáº£i thÃ­ch output:
# JOBID     PARTITION NAME                           ST       TIME  TIME_LIMI  NODES NODELIST(REASON)
# 12345     gpu       train_classical_chinese        R       10:23  2-00:00:00      1 gpu01
```

### Xem Job History

```bash
# Xem jobs trong 7 ngÃ y qua
sacct -u $USER \
      --starttime $(date -d '7 days ago' +%Y-%m-%d) \
      --format=JobID,JobName,State,Elapsed,MaxRSS,MaxVMSize

# Xem chi tiáº¿t má»™t job
sacct -j 12345 --format=JobID,JobName,State,Start,End,Elapsed,MaxRSS,MaxVMSize
```

### Monitor Training Progress

```bash
# Theo dÃµi log realtime
tail -f slurm_logs/train_classical_chinese_12345.out

# Xem 100 dÃ²ng cuá»‘i
tail -n 100 slurm_logs/train_classical_chinese_12345.out

# Grep specific info
grep "Epoch" slurm_logs/train_classical_chinese_12345.out
grep "F1:" slurm_logs/train_classical_chinese_12345.out
grep "Best" slurm_logs/train_classical_chinese_12345.out
```

### Kiá»ƒm tra GPU Usage (náº¿u Ä‘ang cháº¡y interactive)

```bash
# Trong srun session
watch -n 1 nvidia-smi

# Xem GPU memory usage
nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

### Kiá»ƒm tra Checkpoints

```bash
# List checkpoints
ls -lht checkpoints/segmentation/

# Xem thÃ´ng tin checkpoint
python << EOF
import torch
ckpt = torch.load('checkpoints/segmentation/best_model.pt', map_location='cpu')
print(f"Epoch: {ckpt['epoch']}")
print(f"Best F1: {ckpt['best_val_f1']:.4f}")
print(f"Best Epoch: {ckpt['best_epoch']}")
EOF
```

### Cancel Jobs

```bash
# Cancel má»™t job cá»¥ thá»ƒ
scancel 12345

# Cancel táº¥t cáº£ jobs cá»§a báº¡n
scancel -u $USER

# Cancel jobs theo tÃªn
scancel --name=train_classical_chinese
```

### Job Priority & Queue Info

```bash
# Xem vá»‹ trÃ­ job trong queue
squeue -u $USER --start

# Xem thÃ´ng tin partition
sinfo -p gpu

# Xem node availability
sinfo -N -p gpu
```

---

## ğŸ“Š Káº¿t quáº£ Training

### Checkpoints Directory

```
checkpoints/segmentation/
â”œâ”€â”€ best_model.pt              # Model tá»‘t nháº¥t (highest validation F1)
â”œâ”€â”€ latest_checkpoint.pt       # Checkpoint má»›i nháº¥t (Ä‘á»ƒ resume)
â”œâ”€â”€ checkpoint_epoch_5.pt      # Checkpoint Ä‘á»‹nh ká»³ (má»—i 5 epochs)
â”œâ”€â”€ checkpoint_epoch_10.pt
â”œâ”€â”€ checkpoint_epoch_15.pt
â”œâ”€â”€ ...
â”œâ”€â”€ label_config.json          # Label configuration
â””â”€â”€ training_config.json       # Training configuration
```

**Checkpoint Structure:**

```python
{
    'epoch': 45,                           # Epoch hiá»‡n táº¡i
    'model_state_dict': {...},             # Model weights
    'optimizer_state_dict': {...},         # Optimizer state
    'scheduler_state_dict': {...},         # Scheduler state
    'best_val_f1': 0.9234,                # Best validation F1
    'best_epoch': 42,                      # Epoch cá»§a best model
    'patience_counter': 3,                 # Early stopping counter
    'training_config': {...},              # Training configuration
    'model_config': {...}                  # Model configuration
}
```

### Training Logs

```
logs/segmentation/
â”œâ”€â”€ val_metrics_epoch_1.json    # Validation metrics epoch 1
â”œâ”€â”€ val_metrics_epoch_2.json    # Validation metrics epoch 2
â”œâ”€â”€ ...
â”œâ”€â”€ val_metrics_epoch_50.json
â””â”€â”€ test_metrics_final.json     # Final test metrics
```

**Metrics JSON Format:**

```json
{
  "epoch": 45,
  "split": "val",
  "task_type": "segmentation",
  "training_config": {
    "task_type": "segmentation",
    "batch_size": 32,
    "num_epochs": 50,
    "learning_rate": 0.001
  },
  "model_config": {
    "embedding_dim": 128,
    "hidden_dim": 256,
    "num_layers": 2,
    "dropout": 0.3,
    "use_crf": true
  },
  "metrics": {
    "per_class": {
      "B": {
        "precision": 0.9234,
        "recall": 0.9156,
        "f1": 0.9195,
        "support": 15234
      },
      "M": {
        "precision": 0.9123,
        "recall": 0.9087,
        "f1": 0.9105,
        "support": 25123
      },
      "E": {
        "precision": 0.9267,
        "recall": 0.9198,
        "f1": 0.9232,
        "support": 15234
      },
      "S": {
        "precision": 0.8956,
        "recall": 0.8892,
        "f1": 0.8924,
        "support": 4532
      }
    },
    "overall": {
      "precision": 0.9145,
      "recall": 0.9083,
      "f1": 0.9114,
      "total_samples": 60123
    },
    "confusion_matrix": [[...], [...], [...], [...]],
    "label_names": ["B", "M", "E", "S"]
  }
}
```

### Evaluation Results

```
evaluation_results/segmentation/
â”œâ”€â”€ test_metrics.json          # Test metrics (JSON format)
â””â”€â”€ test_samples.txt           # Sample predictions (text format)
```

**Sample Output Format** (`test_samples.txt`):

```
======================================================================
TEST SAMPLES - SEGMENTATION
======================================================================

Sample 1/50 (Index: 1234)
----------------------------------------------------------------------
Char   Pred   True   Status
----------------------------------------
å›      B      B      âœ“
ä¸      M      M      âœ“
è¦‹      E      E      âœ“
å›      B      B      âœ“
æœ‰      M      M      âœ“
ç–¾      E      E      âœ“

Predicted:
|å›ä¸è¦‹||å›æœ‰ç–¾|

Ground Truth:
|å›ä¸è¦‹||å›æœ‰ç–¾|
======================================================================
```

### SLURM Logs

```
slurm_logs/
â”œâ”€â”€ train_classical_chinese_12345.out    # Training stdout
â”œâ”€â”€ train_classical_chinese_12345.err    # Training stderr
â”œâ”€â”€ eval_classical_chinese_12346.out     # Evaluation stdout
â””â”€â”€ eval_classical_chinese_12346.err     # Evaluation stderr
```

---

## ğŸ”§ Troubleshooting

### âŒ Lá»—i: "conda: command not found"

**NguyÃªn nhÃ¢n**: Conda chÆ°a Ä‘Æ°á»£c load vÃ o environment.

**Giáº£i phÃ¡p**:

```bash
# TÃ¬m Ä‘Æ°á»ng dáº«n conda
which conda

# Náº¿u khÃ´ng tÃ¬m tháº¥y, load conda
source /opt/anaconda3/etc/profile.d/conda.sh
# hoáº·c
source $HOME/anaconda3/etc/profile.d/conda.sh
# hoáº·c
source $HOME/miniconda3/etc/profile.d/conda.sh

# Verify
which conda
conda --version
```

### âŒ Lá»—i: "CUDA out of memory"

**NguyÃªn nhÃ¢n**: Batch size quÃ¡ lá»›n cho GPU.

**Giáº£i phÃ¡p**:

```bash
# Option 1: Giáº£m batch size
nano config.slurm
# Thay Ä‘á»•i:
export BATCH_SIZE=16  # hoáº·c 8

# Option 2: Sá»­ dá»¥ng gradient accumulation
# Trong train.py, thÃªm accumulation steps
```

### âŒ Lá»—i: "Job killed due to timeout"

**NguyÃªn nhÃ¢n**: Job vÆ°á»£t quÃ¡ thá»i gian cho phÃ©p (48h).

**Giáº£i phÃ¡p**:

```bash
# Option 1: Giáº£m sá»‘ epochs
export NUM_EPOCHS=30

# Option 2: Resume training tá»« checkpoint
sbatch resume.slurm

# Option 3: TÄƒng batch size Ä‘á»ƒ training nhanh hÆ¡n
export BATCH_SIZE=64  # náº¿u GPU memory cho phÃ©p
```

### âŒ Lá»—i: "No module named 'src'"

**NguyÃªn nhÃ¢n**: Python khÃ´ng tÃ¬m tháº¥y module src.

**Giáº£i phÃ¡p**:

```bash
# Äáº£m báº£o Ä‘ang á»Ÿ Ä‘Ãºng workspace
cd /media02/ddien02/thanhxuan217/main_src

# Kiá»ƒm tra src/ tá»“n táº¡i
ls -la src/

# Kiá»ƒm tra PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:/media02/ddien02/thanhxuan217/main_src"
```

### âŒ Lá»—i: "Job pending quÃ¡ lÃ¢u"

**NguyÃªn nhÃ¢n**: Cluster Ä‘ang báº­n, khÃ´ng Ä‘á»§ resource.

**Giáº£i phÃ¡p**:

```bash
# Xem lÃ½ do pending
squeue -u $USER --start

# Xem node availability
sinfo -p gpu

# Náº¿u cáº§n gáº¥p, cÃ³ thá»ƒ giáº£m resource request
# Sá»­a trong .slurm file:
#SBATCH --gres=gpu:1    # thay vÃ¬ gpu:2
#SBATCH --cpus-per-task=8  # thay vÃ¬ 16
```

### âŒ Lá»—i: "RuntimeError: DataLoader worker exited unexpectedly"

**NguyÃªn nhÃ¢n**: Váº¥n Ä‘á» vá»›i num_workers hoáº·c data corrupted.

**Giáº£i phÃ¡p**:

```bash
# Option 1: Giáº£m num_workers
export NUM_WORKERS=0  # hoáº·c 2

# Option 2: Kiá»ƒm tra data
python << EOF
import json
with open('data/train.jsonl', 'r') as f:
    for i, line in enumerate(f, 1):
        try:
            json.loads(line)
        except:
            print(f"Error at line {i}: {line}")
EOF
```

### âŒ Lá»—i: "FileNotFoundError: checkpoint not found"

**NguyÃªn nhÃ¢n**: Checkpoint file khÃ´ng tá»“n táº¡i.

**Giáº£i phÃ¡p**:

```bash
# Kiá»ƒm tra checkpoints
ls -lh checkpoints/segmentation/

# Náº¿u muá»‘n resume nhÆ°ng khÃ´ng cÃ³ checkpoint, train tá»« Ä‘áº§u
export RESUME_CHECKPOINT=""
sbatch train.slurm
```

### ğŸ” Debug Tips

```bash
# 1. Kiá»ƒm tra SLURM logs
tail -n 50 slurm_logs/train_classical_chinese_*.err

# 2. Test vá»›i small dataset
head -100 data/train.jsonl > data/train_small.jsonl
# Rá»“i test vá»›i data_small

# 3. Enable verbose logging
python train.py --task segmentation ... --log_interval 10

# 4. Kiá»ƒm tra GPU
nvidia-smi
nvidia-smi dmon  # Monitor realtime

# 5. Check disk space
df -h /media02/ddien02/thanhxuan217/
du -sh /media02/ddien02/thanhxuan217/main_src/*
```

---

## ğŸ¯ Examples

### Example 1: Training Segmentation vá»›i CRF (Recommended)

```bash
# Step 1: Edit config.slurm
nano config.slurm
```

```bash
# config.slurm
export TASK="segmentation"
export USE_CRF="true"
export BATCH_SIZE=32
export NUM_EPOCHS=50
export LEARNING_RATE=0.001
export HIDDEN_DIM=256
export NUM_LAYERS=2
```

```bash
# Step 2: Test vá»›i srun
srun --partition=gpu --gres=gpu:1 --cpus-per-task=8 --mem=32G --time=00:30:00 \
     --pty bash -c "
source /opt/anaconda3/etc/profile.d/conda.sh
conda activate /media02/ddien02/thanhxuan217/main_src/envs/classical_chinese
cd /media02/ddien02/thanhxuan217/main_src
python train.py --task segmentation --train_data data/train.jsonl --val_data data/val.jsonl --test_data data/test.jsonl --batch_size 16 --num_epochs 1 --use_crf
"

# Step 3: Náº¿u test OK, submit batch job
sbatch train.slurm

# Step 4: Monitor
squeue -u $USER
tail -f slurm_logs/train_classical_chinese_*.out
```

### Example 2: Training Punctuation vá»›i Linear Head

```bash
# Step 1: Edit config
nano config.slurm
```

```bash
export TASK="punctuation"
export USE_CRF="false"
export BATCH_SIZE=32
export NUM_EPOCHS=50
```

```bash
# Step 2: Submit
sbatch train.slurm

# Step 3: Monitor
tail -f slurm_logs/train_classical_chinese_*.out | grep -E "Epoch|F1|Best"
```

### Example 3: Resume Training sau khi bá»‹ giÃ¡n Ä‘oáº¡n

```bash
# Step 1: Kiá»ƒm tra checkpoint
ls -lh checkpoints/segmentation/latest_checkpoint.pt

# Expected output:
# -rw-r--r-- 1 user group 45M Jan 15 10:30 latest_checkpoint.pt

# Step 2: Xem epoch Ä‘Ã£ train
python << EOF
import torch
ckpt = torch.load('checkpoints/segmentation/latest_checkpoint.pt', map_location='cpu')
print(f"Last completed epoch: {ckpt['epoch']}")
print(f"Best F1 so far: {ckpt['best_val_f1']:.4f}")
print(f"Will resume from epoch: {ckpt['epoch'] + 1}")
EOF

# Step 3: Submit resume job
sbatch resume.slurm

# Step 4: Verify resume
tail -f slurm_logs/resume_train_classical_chinese_*.out | head -20
# Should see: "Resumed from epoch X"
```

### Example 4: Evaluate multiple checkpoints

```bash
# Evaluate best model
python evaluate.py \
    --checkpoint checkpoints/segmentation/best_model.pt \
    --test_data data/test.jsonl \
    --num_samples 100 \
    --output_dir evaluation_results/segmentation/best

# Evaluate specific epoch
python evaluate.py \
    --checkpoint checkpoints/segmentation/checkpoint_epoch_30.pt \
    --test_data data/test.jsonl \
    --num_samples 100 \
    --output_dir evaluation_results/segmentation/epoch30

# Compare results
diff evaluation_results/segmentation/best/test_metrics.json \
     evaluation_results/segmentation/epoch30/test_metrics.json
```

### Example 5: Hyperparameter Search

```bash
# Train with different configurations
for HIDDEN_DIM in 128 256 512; do
    for NUM_LAYERS in 2 3; do
        export HIDDEN_DIM=$HIDDEN_DIM
        export NUM_LAYERS=$NUM_LAYERS
        export SAVE_DIR="checkpoints/segmentation_h${HIDDEN_DIM}_l${NUM_LAYERS}"
        
        # Wait if already have 2 jobs
        while [ $(squeue -u $USER -h | wc -l) -ge 2 ]; do
            echo "Waiting for job slot..."
            sleep 60
        done
        
        sbatch train.slurm
        sleep 5
    done
done
```

---

## ğŸ“š Additional Resources

### Dataset Format Reference

**Segmentation JSONL:**
```jsonl
{"text": "å›å­æœ‰ä¸‰æ¨‚", "labels": ["B", "M", "E", "B", "E"]}
{"text": "çˆ¶æ¯ä¿±å­˜å…„å¼Ÿç„¡æ•…ä¸€æ¨‚ä¹Ÿ", "labels": ["B", "M", "M", "E", "B", "M", "M", "M", "E", "S", "S", "S"]}
```

**Punctuation JSONL:**
```jsonl
{"text": "å­¸è€Œæ™‚ç¿’ä¹‹ä¸äº¦èªªä¹", "labels": ["O", "O", "O", "O", "O", "ï¼Œ", "O", "O", "O", "ï¼Ÿ"]}
{"text": "æœ‰æœ‹è‡ªé æ–¹ä¾†ä¸äº¦æ¨‚ä¹", "labels": ["O", "O", "O", "O", "O", "O", "ï¼Œ", "O", "O", "O", "ï¼Ÿ"]}
```

### Label Schema

**Segmentation (BEMS):**
- `B` (Begin): Token á»Ÿ Ä‘áº§u cÃ¢u
- `M` (Middle): Token á»Ÿ giá»¯a cÃ¢u
- `E` (End): Token á»Ÿ cuá»‘i cÃ¢u
- `S` (Single): CÃ¢u chá»‰ cÃ³ má»™t kÃ½ tá»±

**Punctuation:**
- `O`: KhÃ´ng cÃ³ dáº¥u cÃ¢u
- `ï¼Œ`: Dáº¥u pháº©y
- `ã€‚`: Dáº¥u cháº¥m
- `ï¼š`: Dáº¥u hai cháº¥m
- `ã€`: Dáº¥u Ä‘á»‘t
- `ï¼›`: Dáº¥u cháº¥m pháº©y
- `ï¼Ÿ`: Dáº¥u há»i
- `ï¼`: Dáº¥u cáº£m

### Useful Commands Cheat Sheet

```bash
# === Environment ===
conda activate /media02/ddien02/thanhxuan217/main_src/envs/classical_chinese
conda deactivate

# === Job Management ===
squeue -u $USER                          # My jobs
squeue -u $USER -o "%.18i %.30j %.8T"   # Compact view
sacct -u $USER --starttime today         # Today's history
scancel <JOB_ID>                         # Cancel job
scancel -u $USER                         # Cancel all my jobs

# === Monitoring ===
tail -f slurm_logs/train_*.out          # Follow training log
watch -n 5 'squeue -u $USER'            # Auto-refresh job status
nvidia-smi                               # GPU info

# === Data ===
wc -l data/*.jsonl                      # Count lines
head -5 data/train.jsonl                # View first 5
tail -5 data/train.jsonl                # View last 5

# === Results ===
ls -lht checkpoints/segmentation/       # List checkpoints
cat logs/segmentation/test_metrics_final.json | jq .  # Pretty print JSON
grep "F1" logs/segmentation/val_metrics_*.json        # Extract F1 scores
```

---

## ğŸ“ Support

### Common Issues

1. **Job khÃ´ng cháº¡y**: Kiá»ƒm tra `squeue -u $USER --start`
2. **Out of memory**: Giáº£m batch size
3. **Slow training**: TÄƒng batch size, check GPU usage
4. **Poor performance**: Tune hyperparameters, check data quality

### Getting Help

```bash
# Check cluster documentation
man sbatch
man squeue
man scancel

# Contact support (náº¿u cÃ³)
# Email: support@your-cluster.edu
```

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ™ Acknowledgments

- EvalHan2024 evaluation framework
- PyTorch team
- TorchCRF library

---

**Last Updated**: January 2025  
**Version**: 1.0.0  
**Author**: Classical Chinese NLP Team
